{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import random\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "from pickle import dump,load\n",
    "from random import randint\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Embedding\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "with open(\"moby_dick_four_chapters.txt\") as f:\n",
    "    doc = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading spacy model for preprocessing the text\n",
    "nlp = spacy.load('en_core_web_sm',disable=['parser', 'tagger','ner'])\n",
    "\n",
    "# Gives error if length> 1m so text length needs to be set explicitly\n",
    "nlp.max_length = 1198623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean unnecessary tokens that d cause overfitting\n",
    "\n",
    "def clean_text(doc):\n",
    "    return [token.text.lower() for token in nlp(doc) if token.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = clean_text(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11394"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating sequences of tokens\n",
    "\n",
    "# 25 training words , then one target word\n",
    "train_len = 25+1 \n",
    "\n",
    "text_sequences = []\n",
    "\n",
    "for i in range(train_len, len(tokens)):\n",
    "    \n",
    "    # grabbing train_len 26 words\n",
    "    seq = tokens[i-train_len:i]\n",
    "    \n",
    "    # adding to list of sequences\n",
    "    text_sequences.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11368"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['call',\n",
       " 'me',\n",
       " 'ishmael',\n",
       " 'some',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'never',\n",
       " 'mind',\n",
       " 'how',\n",
       " 'long',\n",
       " 'precisely',\n",
       " 'having',\n",
       " 'little',\n",
       " 'or',\n",
       " 'no',\n",
       " 'money',\n",
       " 'in',\n",
       " 'my',\n",
       " 'purse',\n",
       " 'and',\n",
       " 'nothing',\n",
       " 'particular',\n",
       " 'to',\n",
       " 'interest',\n",
       " 'me',\n",
       " 'on']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encoding words in sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_sequences)\n",
    "sequences = tokenizer.texts_to_sequences(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 964,   14,  265,   51,  263,  416,   87,  222,  129,  111],\n",
       "       [  14,  265,   51,  263,  416,   87,  222,  129,  111,  962],\n",
       "       [ 265,   51,  263,  416,   87,  222,  129,  111,  962,  262],\n",
       "       [  51,  263,  416,   87,  222,  129,  111,  962,  262,   50],\n",
       "       [ 263,  416,   87,  222,  129,  111,  962,  262,   50,   43],\n",
       "       [ 416,   87,  222,  129,  111,  962,  262,   50,   43,   37],\n",
       "       [  87,  222,  129,  111,  962,  262,   50,   43,   37,  321],\n",
       "       [ 222,  129,  111,  962,  262,   50,   43,   37,  321,    7],\n",
       "       [ 129,  111,  962,  262,   50,   43,   37,  321,    7,   23],\n",
       "       [ 111,  962,  262,   50,   43,   37,  321,    7,   23,  555],\n",
       "       [ 962,  262,   50,   43,   37,  321,    7,   23,  555,    3],\n",
       "       [ 262,   50,   43,   37,  321,    7,   23,  555,    3,  150],\n",
       "       [  50,   43,   37,  321,    7,   23,  555,    3,  150,  261],\n",
       "       [  43,   37,  321,    7,   23,  555,    3,  150,  261,    6],\n",
       "       [  37,  321,    7,   23,  555,    3,  150,  261,    6, 2704],\n",
       "       [ 321,    7,   23,  555,    3,  150,  261,    6, 2704,   14],\n",
       "       [   7,   23,  555,    3,  150,  261,    6, 2704,   14,   24],\n",
       "       [  23,  555,    3,  150,  261,    6, 2704,   14,   24,  965],\n",
       "       [ 555,    3,  150,  261,    6, 2704,   14,   24,  965,    5],\n",
       "       [   3,  150,  261,    6, 2704,   14,   24,  965,    5,   60],\n",
       "       [ 150,  261,    6, 2704,   14,   24,  965,    5,   60,    5],\n",
       "       [ 261,    6, 2704,   14,   24,  965,    5,   60,    5,   56],\n",
       "       [   6, 2704,   14,   24,  965,    5,   60,    5,   56,  322],\n",
       "       [2704,   14,   24,  965,    5,   60,    5,   56,  322,   38],\n",
       "       [  14,   24,  965,    5,   60,    5,   56,  322,   38,    2],\n",
       "       [  24,  965,    5,   60,    5,   56,  322,   38,    2,   50]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replaced words in text with particular ids \n",
    "# sequences with shifting one word over\n",
    "sequences = np.array(sequences)\n",
    "sequences[:10].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting training and target sequences\n",
    "X = sequences[:,:-1]\n",
    "y = sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2709"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size = len(tokenizer.word_counts)\n",
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras padding sequences need an extra space to hold zero\n",
    "y = to_categorical(y, num_classes=vocabulary_size+1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = X.shape[1] # setting seq_len = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(vocabulary_size, seq_len):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocabulary_size+1,output_dim=seq_len,input_length=seq_len))\n",
    "    model.add(LSTM(seq_len*2, return_sequences=True))\n",
    "    model.add(LSTM(seq_len*2))\n",
    "    model.add(Dense(seq_len*6, activation='relu'))\n",
    "\n",
    "    model.add(Dense(vocabulary_size+1, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "   \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\WorkPC\\Anaconda3\\envs\\nlp_course\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 25, 25)            67750     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 25, 50)            15200     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 150)               7650      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2710)              409210    \n",
      "=================================================================\n",
      "Total params: 520,010\n",
      "Trainable params: 520,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocabulary_size,seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "11368/11368 [==============================] - 4s 344us/step - loss: 3.9942 - acc: 0.1402\n",
      "Epoch 2/200\n",
      "11368/11368 [==============================] - 4s 392us/step - loss: 3.9528 - acc: 0.1454\n",
      "Epoch 3/200\n",
      "11368/11368 [==============================] - 5s 408us/step - loss: 3.9172 - acc: 0.1529\n",
      "Epoch 4/200\n",
      "11368/11368 [==============================] - 5s 413us/step - loss: 3.8828 - acc: 0.1557\n",
      "Epoch 5/200\n",
      "11368/11368 [==============================] - 5s 413us/step - loss: 3.8373 - acc: 0.1589\n",
      "Epoch 6/200\n",
      "11368/11368 [==============================] - 5s 411us/step - loss: 3.8056 - acc: 0.1657\n",
      "Epoch 7/200\n",
      "11368/11368 [==============================] - 5s 416us/step - loss: 3.7638 - acc: 0.1716\n",
      "Epoch 8/200\n",
      "11368/11368 [==============================] - 5s 420us/step - loss: 3.7222 - acc: 0.1769\n",
      "Epoch 9/200\n",
      "11368/11368 [==============================] - 5s 434us/step - loss: 3.6921 - acc: 0.1792\n",
      "Epoch 10/200\n",
      "11368/11368 [==============================] - 5s 412us/step - loss: 3.6493 - acc: 0.1862\n",
      "Epoch 11/200\n",
      "11368/11368 [==============================] - 5s 416us/step - loss: 3.6072 - acc: 0.1957\n",
      "Epoch 12/200\n",
      "11368/11368 [==============================] - 5s 412us/step - loss: 3.5804 - acc: 0.1979\n",
      "Epoch 13/200\n",
      "11368/11368 [==============================] - 5s 414us/step - loss: 3.5539 - acc: 0.2037\n",
      "Epoch 14/200\n",
      "11368/11368 [==============================] - 5s 412us/step - loss: 3.5083 - acc: 0.2078\n",
      "Epoch 15/200\n",
      "11368/11368 [==============================] - 5s 415us/step - loss: 3.4788 - acc: 0.2149\n",
      "Epoch 16/200\n",
      "11368/11368 [==============================] - 5s 414us/step - loss: 3.4347 - acc: 0.2199\n",
      "Epoch 17/200\n",
      "11368/11368 [==============================] - 5s 413us/step - loss: 3.3955 - acc: 0.2274\n",
      "Epoch 18/200\n",
      "11368/11368 [==============================] - 5s 415us/step - loss: 3.3639 - acc: 0.2343\n",
      "Epoch 19/200\n",
      "11368/11368 [==============================] - 5s 419us/step - loss: 3.3408 - acc: 0.2372\n",
      "Epoch 20/200\n",
      "11368/11368 [==============================] - 5s 417us/step - loss: 3.2963 - acc: 0.2465\n",
      "Epoch 21/200\n",
      "11368/11368 [==============================] - 5s 424us/step - loss: 3.2609 - acc: 0.2538\n",
      "Epoch 22/200\n",
      "11368/11368 [==============================] - 5s 435us/step - loss: 3.2289 - acc: 0.2565 0s - loss: 3.2261 - acc: 0.2\n",
      "Epoch 23/200\n",
      "11368/11368 [==============================] - 5s 436us/step - loss: 3.1948 - acc: 0.2642\n",
      "Epoch 24/200\n",
      "11368/11368 [==============================] - 5s 422us/step - loss: 3.1554 - acc: 0.2725\n",
      "Epoch 25/200\n",
      "11368/11368 [==============================] - 5s 423us/step - loss: 3.1423 - acc: 0.2721\n",
      "Epoch 26/200\n",
      "11368/11368 [==============================] - 5s 429us/step - loss: 3.1208 - acc: 0.2734\n",
      "Epoch 27/200\n",
      "11368/11368 [==============================] - 5s 427us/step - loss: 3.0758 - acc: 0.2793\n",
      "Epoch 28/200\n",
      "11368/11368 [==============================] - 5s 430us/step - loss: 3.0366 - acc: 0.2927\n",
      "Epoch 29/200\n",
      "11368/11368 [==============================] - 5s 438us/step - loss: 3.0177 - acc: 0.2950 0s - loss: 2.992\n",
      "Epoch 30/200\n",
      "11368/11368 [==============================] - 5s 435us/step - loss: 2.9708 - acc: 0.3044\n",
      "Epoch 31/200\n",
      "11368/11368 [==============================] - 5s 433us/step - loss: 2.9495 - acc: 0.3059\n",
      "Epoch 32/200\n",
      "11368/11368 [==============================] - 5s 435us/step - loss: 2.9107 - acc: 0.3130\n",
      "Epoch 33/200\n",
      "11368/11368 [==============================] - 5s 434us/step - loss: 2.8849 - acc: 0.3199\n",
      "Epoch 34/200\n",
      "11368/11368 [==============================] - 5s 438us/step - loss: 2.8516 - acc: 0.3286\n",
      "Epoch 35/200\n",
      "11368/11368 [==============================] - 5s 444us/step - loss: 2.8236 - acc: 0.3312\n",
      "Epoch 36/200\n",
      "11368/11368 [==============================] - 5s 467us/step - loss: 2.7855 - acc: 0.3389\n",
      "Epoch 37/200\n",
      "11368/11368 [==============================] - 5s 445us/step - loss: 2.7595 - acc: 0.3410 0s - loss: 2.739\n",
      "Epoch 38/200\n",
      "11368/11368 [==============================] - 5s 451us/step - loss: 2.7397 - acc: 0.3455\n",
      "Epoch 39/200\n",
      "11368/11368 [==============================] - 5s 454us/step - loss: 2.7105 - acc: 0.3534\n",
      "Epoch 40/200\n",
      "11368/11368 [==============================] - 5s 455us/step - loss: 2.6832 - acc: 0.3577\n",
      "Epoch 41/200\n",
      "11368/11368 [==============================] - 5s 463us/step - loss: 2.6471 - acc: 0.3673\n",
      "Epoch 42/200\n",
      "11368/11368 [==============================] - 5s 461us/step - loss: 2.6213 - acc: 0.3729\n",
      "Epoch 43/200\n",
      "11368/11368 [==============================] - 5s 464us/step - loss: 2.5984 - acc: 0.3742 3s \n",
      "Epoch 44/200\n",
      "11368/11368 [==============================] - 5s 468us/step - loss: 2.5694 - acc: 0.3834\n",
      "Epoch 45/200\n",
      "11368/11368 [==============================] - 5s 471us/step - loss: 2.5461 - acc: 0.3878\n",
      "Epoch 46/200\n",
      "11368/11368 [==============================] - 5s 480us/step - loss: 2.5229 - acc: 0.3907\n",
      "Epoch 47/200\n",
      "11368/11368 [==============================] - 5s 480us/step - loss: 2.5015 - acc: 0.3920\n",
      "Epoch 48/200\n",
      "11368/11368 [==============================] - 6s 505us/step - loss: 2.4669 - acc: 0.4046\n",
      "Epoch 49/200\n",
      "11368/11368 [==============================] - 6s 494us/step - loss: 2.4467 - acc: 0.4060\n",
      "Epoch 50/200\n",
      "11368/11368 [==============================] - 6s 498us/step - loss: 2.4112 - acc: 0.4159\n",
      "Epoch 51/200\n",
      "11368/11368 [==============================] - 6s 494us/step - loss: 2.3919 - acc: 0.4147\n",
      "Epoch 52/200\n",
      "11368/11368 [==============================] - 6s 499us/step - loss: 2.3736 - acc: 0.4214\n",
      "Epoch 53/200\n",
      "11368/11368 [==============================] - 6s 499us/step - loss: 2.3449 - acc: 0.4299\n",
      "Epoch 54/200\n",
      "11368/11368 [==============================] - 6s 504us/step - loss: 2.3243 - acc: 0.4280\n",
      "Epoch 55/200\n",
      "11368/11368 [==============================] - 6s 506us/step - loss: 2.3077 - acc: 0.4380\n",
      "Epoch 56/200\n",
      "11368/11368 [==============================] - 6s 507us/step - loss: 2.2880 - acc: 0.4388\n",
      "Epoch 57/200\n",
      "11368/11368 [==============================] - 6s 508us/step - loss: 2.2665 - acc: 0.4405\n",
      "Epoch 58/200\n",
      "11368/11368 [==============================] - 6s 517us/step - loss: 2.2470 - acc: 0.4446\n",
      "Epoch 59/200\n",
      "11368/11368 [==============================] - 6s 528us/step - loss: 2.1968 - acc: 0.4624\n",
      "Epoch 60/200\n",
      "11368/11368 [==============================] - 6s 533us/step - loss: 2.1849 - acc: 0.4600\n",
      "Epoch 61/200\n",
      "11368/11368 [==============================] - 6s 523us/step - loss: 2.1653 - acc: 0.4679\n",
      "Epoch 62/200\n",
      "11368/11368 [==============================] - 6s 529us/step - loss: 2.1412 - acc: 0.4671\n",
      "Epoch 63/200\n",
      "11368/11368 [==============================] - 6s 532us/step - loss: 2.1140 - acc: 0.4762\n",
      "Epoch 64/200\n",
      "11368/11368 [==============================] - 6s 535us/step - loss: 2.0996 - acc: 0.4795\n",
      "Epoch 65/200\n",
      "11368/11368 [==============================] - 6s 540us/step - loss: 2.0951 - acc: 0.4799\n",
      "Epoch 66/200\n",
      "11368/11368 [==============================] - 6s 542us/step - loss: 2.0629 - acc: 0.4865\n",
      "Epoch 67/200\n",
      "11368/11368 [==============================] - 6s 546us/step - loss: 2.0442 - acc: 0.4904\n",
      "Epoch 68/200\n",
      "11368/11368 [==============================] - 6s 554us/step - loss: 2.0177 - acc: 0.4943\n",
      "Epoch 69/200\n",
      "11368/11368 [==============================] - 6s 555us/step - loss: 2.0070 - acc: 0.4981\n",
      "Epoch 70/200\n",
      "11368/11368 [==============================] - 7s 573us/step - loss: 1.9900 - acc: 0.5011\n",
      "Epoch 71/200\n",
      "11368/11368 [==============================] - 6s 571us/step - loss: 1.9640 - acc: 0.5104\n",
      "Epoch 72/200\n",
      "11368/11368 [==============================] - 7s 577us/step - loss: 1.9429 - acc: 0.5106\n",
      "Epoch 73/200\n",
      "11368/11368 [==============================] - 7s 574us/step - loss: 1.9195 - acc: 0.5151\n",
      "Epoch 74/200\n",
      "11368/11368 [==============================] - 7s 576us/step - loss: 1.9076 - acc: 0.5201\n",
      "Epoch 75/200\n",
      "11368/11368 [==============================] - 7s 580us/step - loss: 1.8865 - acc: 0.5274\n",
      "Epoch 76/200\n",
      "11368/11368 [==============================] - 7s 583us/step - loss: 1.8641 - acc: 0.5318\n",
      "Epoch 77/200\n",
      "11368/11368 [==============================] - 7s 588us/step - loss: 1.8526 - acc: 0.5321\n",
      "Epoch 78/200\n",
      "11368/11368 [==============================] - 7s 591us/step - loss: 1.8277 - acc: 0.5406\n",
      "Epoch 79/200\n",
      "11368/11368 [==============================] - 7s 597us/step - loss: 1.8231 - acc: 0.5398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/200\n",
      "11368/11368 [==============================] - 7s 612us/step - loss: 1.8154 - acc: 0.5397\n",
      "Epoch 81/200\n",
      "11368/11368 [==============================] - 7s 602us/step - loss: 1.7832 - acc: 0.5436\n",
      "Epoch 82/200\n",
      "11368/11368 [==============================] - 7s 611us/step - loss: 1.7586 - acc: 0.5515\n",
      "Epoch 83/200\n",
      "11368/11368 [==============================] - 7s 606us/step - loss: 1.7479 - acc: 0.5530\n",
      "Epoch 84/200\n",
      "11368/11368 [==============================] - 7s 614us/step - loss: 1.7087 - acc: 0.5677\n",
      "Epoch 85/200\n",
      "11368/11368 [==============================] - 7s 620us/step - loss: 1.7098 - acc: 0.5698\n",
      "Epoch 86/200\n",
      "11368/11368 [==============================] - 7s 621us/step - loss: 1.6959 - acc: 0.5696\n",
      "Epoch 87/200\n",
      "11368/11368 [==============================] - 7s 622us/step - loss: 1.6770 - acc: 0.5713\n",
      "Epoch 88/200\n",
      "11368/11368 [==============================] - 7s 623us/step - loss: 1.6617 - acc: 0.5800\n",
      "Epoch 89/200\n",
      "11368/11368 [==============================] - 7s 642us/step - loss: 1.6429 - acc: 0.5838\n",
      "Epoch 90/200\n",
      "11368/11368 [==============================] - 7s 653us/step - loss: 1.6201 - acc: 0.5844\n",
      "Epoch 91/200\n",
      "11368/11368 [==============================] - 7s 640us/step - loss: 1.6091 - acc: 0.5892\n",
      "Epoch 92/200\n",
      "11368/11368 [==============================] - 8s 671us/step - loss: 1.6072 - acc: 0.5899 2s - lo\n",
      "Epoch 93/200\n",
      "11368/11368 [==============================] - 8s 687us/step - loss: 1.5896 - acc: 0.5939\n",
      "Epoch 94/200\n",
      "11368/11368 [==============================] - 8s 675us/step - loss: 1.5647 - acc: 0.5988\n",
      "Epoch 95/200\n",
      "11368/11368 [==============================] - 8s 687us/step - loss: 1.5523 - acc: 0.6017\n",
      "Epoch 96/200\n",
      "11368/11368 [==============================] - 8s 686us/step - loss: 1.5372 - acc: 0.6035\n",
      "Epoch 97/200\n",
      "11368/11368 [==============================] - 8s 683us/step - loss: 1.5371 - acc: 0.6059\n",
      "Epoch 98/200\n",
      "11368/11368 [==============================] - 8s 711us/step - loss: 1.5077 - acc: 0.6138\n",
      "Epoch 99/200\n",
      "11368/11368 [==============================] - 8s 687us/step - loss: 1.4899 - acc: 0.6217 4s - loss: 1.4243 - ac\n",
      "Epoch 100/200\n",
      "11368/11368 [==============================] - 8s 690us/step - loss: 1.4752 - acc: 0.6220\n",
      "Epoch 101/200\n",
      "11368/11368 [==============================] - 8s 691us/step - loss: 1.4528 - acc: 0.6270 0s - loss: 1.4351 - acc: - ETA: 0s - loss: 1.4477 - acc: 0.\n",
      "Epoch 102/200\n",
      "11368/11368 [==============================] - 8s 707us/step - loss: 1.4429 - acc: 0.6298 1s - loss: \n",
      "Epoch 103/200\n",
      "11368/11368 [==============================] - 8s 705us/step - loss: 1.4449 - acc: 0.6317 6s - loss: 1.4016  - ETA: 1s - loss: \n",
      "Epoch 104/200\n",
      "11368/11368 [==============================] - 8s 708us/step - loss: 1.4376 - acc: 0.6299\n",
      "Epoch 105/200\n",
      "11368/11368 [==============================] - 8s 708us/step - loss: 1.4069 - acc: 0.6373\n",
      "Epoch 106/200\n",
      "11368/11368 [==============================] - 8s 720us/step - loss: 1.3851 - acc: 0.6431\n",
      "Epoch 107/200\n",
      "11368/11368 [==============================] - 8s 732us/step - loss: 1.3666 - acc: 0.6492\n",
      "Epoch 108/200\n",
      "11368/11368 [==============================] - 8s 745us/step - loss: 1.3682 - acc: 0.6495\n",
      "Epoch 109/200\n",
      "11368/11368 [==============================] - 8s 733us/step - loss: 1.3425 - acc: 0.6565\n",
      "Epoch 110/200\n",
      "11368/11368 [==============================] - 8s 735us/step - loss: 1.3286 - acc: 0.6587\n",
      "Epoch 111/200\n",
      "11368/11368 [==============================] - 8s 740us/step - loss: 1.3251 - acc: 0.6567\n",
      "Epoch 112/200\n",
      "11368/11368 [==============================] - 8s 741us/step - loss: 1.3185 - acc: 0.6577\n",
      "Epoch 113/200\n",
      "11368/11368 [==============================] - 9s 751us/step - loss: 1.3052 - acc: 0.6617 3s - loss: 1.2575 - a - ETA: 2s \n",
      "Epoch 114/200\n",
      "11368/11368 [==============================] - 9s 793us/step - loss: 1.2925 - acc: 0.6670\n",
      "Epoch 115/200\n",
      "11368/11368 [==============================] - 9s 754us/step - loss: 1.2630 - acc: 0.6787\n",
      "Epoch 116/200\n",
      "11368/11368 [==============================] - 9s 760us/step - loss: 1.2394 - acc: 0.6827\n",
      "Epoch 117/200\n",
      "11368/11368 [==============================] - 9s 762us/step - loss: 1.2303 - acc: 0.6846\n",
      "Epoch 118/200\n",
      "11368/11368 [==============================] - 9s 763us/step - loss: 1.2296 - acc: 0.6821\n",
      "Epoch 119/200\n",
      "11368/11368 [==============================] - 9s 771us/step - loss: 1.2213 - acc: 0.6844\n",
      "Epoch 120/200\n",
      "11368/11368 [==============================] - 9s 775us/step - loss: 1.2159 - acc: 0.6853\n",
      "Epoch 121/200\n",
      "11368/11368 [==============================] - 9s 791us/step - loss: 1.2032 - acc: 0.6891\n",
      "Epoch 122/200\n",
      "11368/11368 [==============================] - 9s 797us/step - loss: 1.2057 - acc: 0.6876\n",
      "Epoch 123/200\n",
      "11368/11368 [==============================] - 9s 783us/step - loss: 1.1819 - acc: 0.6989\n",
      "Epoch 124/200\n",
      "11368/11368 [==============================] - 9s 773us/step - loss: 1.1505 - acc: 0.7045\n",
      "Epoch 125/200\n",
      "11368/11368 [==============================] - 9s 784us/step - loss: 1.1406 - acc: 0.7058\n",
      "Epoch 126/200\n",
      "11368/11368 [==============================] - 9s 770us/step - loss: 1.1264 - acc: 0.7109\n",
      "Epoch 127/200\n",
      "11368/11368 [==============================] - 9s 783us/step - loss: 1.1247 - acc: 0.7102\n",
      "Epoch 128/200\n",
      "11368/11368 [==============================] - 9s 792us/step - loss: 1.1111 - acc: 0.7116\n",
      "Epoch 129/200\n",
      "11368/11368 [==============================] - 9s 804us/step - loss: 1.0876 - acc: 0.7174\n",
      "Epoch 130/200\n",
      "11368/11368 [==============================] - 9s 795us/step - loss: 1.0718 - acc: 0.7232\n",
      "Epoch 131/200\n",
      "11368/11368 [==============================] - 9s 795us/step - loss: 1.0664 - acc: 0.7242\n",
      "Epoch 132/200\n",
      "11368/11368 [==============================] - 9s 799us/step - loss: 1.0621 - acc: 0.7279\n",
      "Epoch 133/200\n",
      "11368/11368 [==============================] - 9s 800us/step - loss: 1.0628 - acc: 0.7257\n",
      "Epoch 134/200\n",
      "11368/11368 [==============================] - 9s 811us/step - loss: 1.0474 - acc: 0.7314\n",
      "Epoch 135/200\n",
      "11368/11368 [==============================] - 9s 806us/step - loss: 1.0314 - acc: 0.7315\n",
      "Epoch 136/200\n",
      "11368/11368 [==============================] - 9s 821us/step - loss: 1.0124 - acc: 0.7402\n",
      "Epoch 137/200\n",
      "11368/11368 [==============================] - 9s 810us/step - loss: 1.0112 - acc: 0.7429\n",
      "Epoch 138/200\n",
      "11368/11368 [==============================] - 9s 807us/step - loss: 0.9997 - acc: 0.7415\n",
      "Epoch 139/200\n",
      "11368/11368 [==============================] - 9s 812us/step - loss: 0.9927 - acc: 0.7436\n",
      "Epoch 140/200\n",
      "11368/11368 [==============================] - 9s 813us/step - loss: 0.9795 - acc: 0.7424\n",
      "Epoch 141/200\n",
      "11368/11368 [==============================] - 9s 819us/step - loss: 0.9738 - acc: 0.7471\n",
      "Epoch 142/200\n",
      "11368/11368 [==============================] - 9s 820us/step - loss: 0.9366 - acc: 0.7626\n",
      "Epoch 143/200\n",
      "11368/11368 [==============================] - 10s 837us/step - loss: 0.9201 - acc: 0.7712\n",
      "Epoch 144/200\n",
      "11368/11368 [==============================] - 9s 823us/step - loss: 0.9139 - acc: 0.7672\n",
      "Epoch 145/200\n",
      "11368/11368 [==============================] - 9s 823us/step - loss: 0.9090 - acc: 0.7665\n",
      "Epoch 146/200\n",
      "11368/11368 [==============================] - 9s 828us/step - loss: 0.9053 - acc: 0.7713\n",
      "Epoch 147/200\n",
      "11368/11368 [==============================] - 9s 832us/step - loss: 0.9018 - acc: 0.7696\n",
      "Epoch 148/200\n",
      "11368/11368 [==============================] - 10s 879us/step - loss: 0.9035 - acc: 0.7685\n",
      "Epoch 149/200\n",
      "11368/11368 [==============================] - 10s 854us/step - loss: 0.8818 - acc: 0.7741\n",
      "Epoch 150/200\n",
      "11368/11368 [==============================] - 10s 870us/step - loss: 0.8709 - acc: 0.7734\n",
      "Epoch 151/200\n",
      "11368/11368 [==============================] - 10s 843us/step - loss: 0.8644 - acc: 0.7792\n",
      "Epoch 152/200\n",
      "11368/11368 [==============================] - 9s 834us/step - loss: 0.8479 - acc: 0.7858\n",
      "Epoch 153/200\n",
      "11368/11368 [==============================] - 9s 829us/step - loss: 0.8342 - acc: 0.7874\n",
      "Epoch 154/200\n",
      "11368/11368 [==============================] - 10s 841us/step - loss: 0.8216 - acc: 0.7906\n",
      "Epoch 155/200\n",
      "11368/11368 [==============================] - 10s 853us/step - loss: 0.8196 - acc: 0.7930\n",
      "Epoch 156/200\n",
      "11368/11368 [==============================] - 10s 859us/step - loss: 0.8176 - acc: 0.7948\n",
      "Epoch 157/200\n",
      "11368/11368 [==============================] - 10s 855us/step - loss: 0.8095 - acc: 0.7962\n",
      "Epoch 158/200\n",
      "11368/11368 [==============================] - 9s 833us/step - loss: 0.7847 - acc: 0.7998\n",
      "Epoch 159/200\n",
      "11368/11368 [==============================] - 10s 861us/step - loss: 0.7775 - acc: 0.8069\n",
      "Epoch 160/200\n",
      "11368/11368 [==============================] - 10s 856us/step - loss: 0.7511 - acc: 0.8118\n",
      "Epoch 161/200\n",
      "11368/11368 [==============================] - 10s 847us/step - loss: 0.7830 - acc: 0.7993\n",
      "Epoch 162/200\n",
      "11368/11368 [==============================] - 10s 851us/step - loss: 0.7714 - acc: 0.8023\n",
      "Epoch 163/200\n",
      "11368/11368 [==============================] - 10s 848us/step - loss: 0.7464 - acc: 0.8120\n",
      "Epoch 164/200\n",
      "11368/11368 [==============================] - 10s 867us/step - loss: 0.7290 - acc: 0.8142\n",
      "Epoch 165/200\n",
      "11368/11368 [==============================] - 10s 851us/step - loss: 0.7186 - acc: 0.8199\n",
      "Epoch 166/200\n",
      "11368/11368 [==============================] - 10s 858us/step - loss: 0.6998 - acc: 0.8246\n",
      "Epoch 167/200\n",
      "11368/11368 [==============================] - 10s 865us/step - loss: 0.6867 - acc: 0.8288\n",
      "Epoch 168/200\n",
      "11368/11368 [==============================] - 10s 869us/step - loss: 0.6772 - acc: 0.8344\n",
      "Epoch 169/200\n",
      "11368/11368 [==============================] - 10s 850us/step - loss: 0.6926 - acc: 0.8277\n",
      "Epoch 170/200\n",
      "11368/11368 [==============================] - 10s 865us/step - loss: 0.6874 - acc: 0.8286\n",
      "Epoch 171/200\n",
      "11368/11368 [==============================] - 10s 863us/step - loss: 0.6748 - acc: 0.8309\n",
      "Epoch 172/200\n",
      "11368/11368 [==============================] - 10s 865us/step - loss: 0.6774 - acc: 0.8317\n",
      "Epoch 173/200\n",
      "11368/11368 [==============================] - 10s 878us/step - loss: 0.6752 - acc: 0.8293\n",
      "Epoch 174/200\n",
      "11368/11368 [==============================] - 10s 872us/step - loss: 0.6547 - acc: 0.8360\n",
      "Epoch 175/200\n",
      "11368/11368 [==============================] - 10s 871us/step - loss: 0.6314 - acc: 0.8454\n",
      "Epoch 176/200\n",
      "11368/11368 [==============================] - 10s 872us/step - loss: 0.6179 - acc: 0.8492\n",
      "Epoch 177/200\n",
      "11368/11368 [==============================] - 10s 885us/step - loss: 0.6184 - acc: 0.8474\n",
      "Epoch 178/200\n",
      "11368/11368 [==============================] - 10s 870us/step - loss: 0.6160 - acc: 0.8484\n",
      "Epoch 179/200\n",
      "11368/11368 [==============================] - 10s 877us/step - loss: 0.6126 - acc: 0.8495\n",
      "Epoch 180/200\n",
      "11368/11368 [==============================] - 10s 870us/step - loss: 0.5960 - acc: 0.8536\n",
      "Epoch 181/200\n",
      "11368/11368 [==============================] - 10s 867us/step - loss: 0.5906 - acc: 0.8550\n",
      "Epoch 182/200\n",
      "11368/11368 [==============================] - 10s 875us/step - loss: 0.5871 - acc: 0.8548\n",
      "Epoch 183/200\n",
      "11368/11368 [==============================] - 10s 873us/step - loss: 0.5923 - acc: 0.8505\n",
      "Epoch 184/200\n",
      "11368/11368 [==============================] - 10s 887us/step - loss: 0.6065 - acc: 0.8471\n",
      "Epoch 185/200\n",
      "11368/11368 [==============================] - 10s 872us/step - loss: 0.5841 - acc: 0.8581\n",
      "Epoch 186/200\n",
      "11368/11368 [==============================] - 10s 883us/step - loss: 0.5604 - acc: 0.8625\n",
      "Epoch 187/200\n",
      "11368/11368 [==============================] - 10s 888us/step - loss: 0.5463 - acc: 0.8691\n",
      "Epoch 188/200\n",
      "11368/11368 [==============================] - 10s 887us/step - loss: 0.5374 - acc: 0.8694\n",
      "Epoch 189/200\n",
      "11368/11368 [==============================] - 10s 895us/step - loss: 0.5447 - acc: 0.8675\n",
      "Epoch 190/200\n",
      "11368/11368 [==============================] - 11s 929us/step - loss: 0.5705 - acc: 0.8556\n",
      "Epoch 191/200\n",
      "11368/11368 [==============================] - 10s 880us/step - loss: 0.5475 - acc: 0.8640\n",
      "Epoch 192/200\n",
      "11368/11368 [==============================] - 10s 893us/step - loss: 0.5521 - acc: 0.8613\n",
      "Epoch 193/200\n",
      "11368/11368 [==============================] - 10s 886us/step - loss: 0.5292 - acc: 0.8695\n",
      "Epoch 194/200\n",
      "11368/11368 [==============================] - 10s 886us/step - loss: 0.4922 - acc: 0.8817\n",
      "Epoch 195/200\n",
      "11368/11368 [==============================] - 10s 911us/step - loss: 0.4737 - acc: 0.8878\n",
      "Epoch 196/200\n",
      "11368/11368 [==============================] - 10s 917us/step - loss: 0.4525 - acc: 0.8985\n",
      "Epoch 197/200\n",
      "11368/11368 [==============================] - 10s 918us/step - loss: 0.4566 - acc: 0.8941\n",
      "Epoch 198/200\n",
      "11368/11368 [==============================] - 10s 904us/step - loss: 0.4652 - acc: 0.8911\n",
      "Epoch 199/200\n",
      "11368/11368 [==============================] - 11s 931us/step - loss: 0.4386 - acc: 0.8998\n",
      "Epoch 200/200\n",
      "11368/11368 [==============================] - 11s 934us/step - loss: 0.4412 - acc: 0.8995\n"
     ]
    }
   ],
   "source": [
    "# fit model\n",
    "history = model.fit(X, y, batch_size=128, epochs=200,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "model.save('epoch_200.h5')\n",
    "\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text = ' '.join(text_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'call me ishmael some years ago never mind how long precisely having little or no money in my purse and nothing particular to interest me on'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[964,\n",
       "  14,\n",
       "  265,\n",
       "  51,\n",
       "  263,\n",
       "  416,\n",
       "  87,\n",
       "  222,\n",
       "  129,\n",
       "  111,\n",
       "  962,\n",
       "  262,\n",
       "  50,\n",
       "  43,\n",
       "  37,\n",
       "  321,\n",
       "  7,\n",
       "  23,\n",
       "  555,\n",
       "  3,\n",
       "  150,\n",
       "  261,\n",
       "  6,\n",
       "  2704,\n",
       "  14,\n",
       "  24]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([seed_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate specific number of words followings a seed text\n",
    "\n",
    "def generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n",
    "\n",
    "    output_text = []\n",
    "    \n",
    "    for i in range(num_gen_words):\n",
    "        \n",
    "        encoded_text = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\n",
    "        pred_word_ind = model.predict_classes(pad_encoded, verbose=0)[0]\n",
    "        pred_word = tokenizer.index_word[pred_word_ind] \n",
    "        \n",
    "        seed_text += ' ' + pred_word\n",
    "        \n",
    "        output_text.append(pred_word)\n",
    "        \n",
    "    return ' '.join(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shore i thought i would sail about you inquire the heavy footfall of the passage and you her harpooneer would be not seen i be'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
